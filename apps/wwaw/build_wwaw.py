from argparse import ArgumentParser
from hashlib import md5
from os import makedirs
from random import choices
from random import uniform
from re import sub
from string import ascii_lowercase
from string import digits
from time import sleep
from typing import List
from typing import Optional
from typing import Tuple
from urllib.parse import urljoin
from urllib.parse import urlparse

from bs4 import BeautifulSoup
from hocon_constants import HOCON_HEADER_REMAINDER
from hocon_constants import HOCON_HEADER_START
from hocon_constants import LEAF_NODE_AGENT_TEMPLATE
from hocon_constants import REGULAR_AGENT_TEMPLATE
from hocon_constants import TOP_AGENT_TEMPLATE
from requests import RequestException
from requests import get
from tldextract import extract

# Regex to replace all non-alphanumeric and non-hyphen characters with an empty string
SAFE_AGENT_NAME_CHARS_REGEX = r"[^a-zA-Z0-9\-]"

# Regex to replace all non-word, non-space, and non-hyphen characters with a space
AGENT_NAME_SANITIZE_REGEX = r"[^\w\s-]"

# Regex to replace sequences of whitespace or underscores with a single hyphen
AGENT_NAME_HYPHENATE_REGEX = r"[\s_]+"

# Regex to remove URLs from extracted text
URL_REGEX = r"https?://\S+"

# Regex to remove scene7 junk or custom format @(...) from extracted text
SCENE7_JUNK_REGEX = r"@\(.*?\)"


class WebAgentNetworkBuilder:
    TOTAL_AGENTS = 40
    MAX_CHILDREN = 10
    MAX_NAME_LEN = 40  # Cannot be more than 55
    PAGE_LEN_MAX = 5000
    MIN_PAGE_LEN = 200
    START_URL = "https://www.cognizant.com/us/en"
    AGENT_NETWORK_NAME = f"autogenerated_agent_network_{TOTAL_AGENTS}"
    OUTPUT_PATH = "../../registries/"  # Make sure the new hocon is added to the manifest
    AGENT_INSTRUCTION_PREFACE = (
        "You represent the following content from a web page for the user and can answer "
        "questions related to this content. "
        "The web page you represent is part of a larger web site, represented by your tools. "
        "Always use your tools as an extension of your content. Your content might not be up "
        "to date or complete, but your tools may have the latest or complementary information,"
        " so always check with your tools too. "
        "Make sure you give the user all the information you have in all the content you and "
        "your tools represent."
        "Here is the specific content that you represent and the remainder is represented by "
        "your tools: "
    )

    def __init__(self):
        self.agent_counter = 0
        self.politeness_delay = 0.0
        self.top_agent_name = None

    def create_intermediate_agents(self, parent: str, chunks: List[List[str]], new_agents: dict) -> List[str]:
        """
        Creates intermediate agents to group subsets of children when fan-out exceeds max_children.
        Returns the list of new intermediate agent names.
        """
        intermediate_names = []
        for idx, chunk in enumerate(chunks):
            intermediate_name = f"{parent}_branch_{idx}"
            intermediate_name = sub(SAFE_AGENT_NAME_CHARS_REGEX, "", intermediate_name).lower()

            while intermediate_name == parent or intermediate_name in new_agents:
                idx += 1
                intermediate_name = f"{parent}_branch_{idx}"
                intermediate_name = sub(SAFE_AGENT_NAME_CHARS_REGEX, "", intermediate_name).lower()

            instructions = (
                (f"{self.AGENT_INSTRUCTION_PREFACE} You are an intermediate agent, grouping {len(chunk)} sub-agents.")
                .replace('"', "")
                .replace("'", "")
            )

            new_agents[intermediate_name] = {"instructions": instructions, "down_chains": chunk, "top_agent": "false"}

            intermediate_names.append(intermediate_name)
        return intermediate_names

    def enforce_max_fanout(self, agents: dict, max_children: int = None) -> dict:
        """
        Ensures that no agent in the given agent hierarchy has more than `max_children` direct children.

        If an agent exceeds the allowed fan-out, intermediate agents (branches) are created to group
        subsets of its children. These intermediate agents are inserted into the hierarchy, and the
        original agent's down_chains are replaced with references to the new intermediate agents.

        Args:
            agents (dict): A dictionary representing the agent hierarchy. Each key is an agent name,
                           and each value is a dictionary with "instructions", "down_chains", and "top_agent".
            max_children (int): Maximum number of direct children allowed per agent.

        Returns:
            dict: A new dictionary with the same structure as `agents` but with fan-out constraints enforced.
        """
        if max_children is None:
            max_children = self.MAX_CHILDREN
        new_agents = dict(agents)  # Shallow copy is safe here

        for parent, data in list(agents.items()):
            children = data.get("down_chains", [])
            if len(children) <= max_children:
                continue

            # Split into chunks using explicit loop
            chunks = []
            for i in range(0, len(children), max_children):
                chunks.append(children[i : i + max_children])

            chunks = [children[i : i + max_children] for i in range(0, len(children), max_children)]
            intermediate_names = self.create_intermediate_agents(parent, chunks, new_agents)

            # Overwrite the parent's down_chains with the new intermediate branches
            new_agents[parent]["down_chains"] = intermediate_names
        return new_agents

    def enforce_fanout_recursive(self, agents, max_children=None):
        """
        Recursively enforces the maximum fan-out constraint on a hierarchy of agents.

        Applies `enforce_max_fanout` repeatedly until all agents in the hierarchy have no more than
        `max_children` direct children. This is necessary when intermediate agents introduced by a
        previous enforcement may themselves exceed the limit.

        Args:
            agents (dict): A dictionary representing the agent hierarchy.
            max_children (int): Maximum number of allowed direct children per agent.

        Returns:
            dict: A modified agent hierarchy with fan-out constraints fully enforced.
        """
        # Use class default if no specific max_children value is provided
        if max_children is None:
            max_children = self.MAX_CHILDREN
        # Perform an initial fan-out enforcement pass
        updated_agents = self.enforce_max_fanout(agents, max_children)
        # Repeat enforcement until no agent exceeds max_children
        has_overflow = True
        while has_overflow:
            has_overflow = False
            # Check each agent to see if fan-out exceeds limit
            for agent in updated_agents.values():
                if len(agent["down_chains"]) > max_children:
                    has_overflow = True
                    break
            if has_overflow:
                # Reassign agents and run another pass of enforcement
                agents = updated_agents
                updated_agents = self.enforce_max_fanout(agents, max_children)
        return updated_agents

    def add_agent(self, agents, agent_name: str, instructions: str, down_chains: list, top_agent: str = "false"):
        """
        Adds a new agent to the agent hierarchy with the specified attributes.

        Validates that the agent name is unique and does not self-reference in its down_chains.
        Updates the global agent counter and prints progress indicators every 100 and 5000 agents.

        Args:
            agents (dict): The dictionary storing the agent hierarchy.
            agent_name (str): Unique identifier for the new agent.
            instructions (str): Instructional content assigned to the agent.
            down_chains (list): List of names of agents that this agent links to as sub-agents.
            top_agent (str): String flag ("true" or "false") indicating whether this is the top agent in the network.

        Returns:
            str: A string representation of the newly added agent's data.
        """
        if agent_name in agents:
            raise ValueError(f"Duplicate agent: '{agent_name}' already exists.")
        if agent_name in down_chains or down_chains is None:
            raise ValueError(f"Self-reference detected: '{agent_name}' cannot have itself as child.")
        agents[agent_name] = {
            "instructions": instructions,
            "down_chains": down_chains,
            "top_agent": top_agent,
        }
        self.agent_counter += 1
        if self.agent_counter % 100 == 0:
            print(".", end="", flush=True)
        if self.agent_counter % 5000 == 0:
            print(f" {self.agent_counter}")
        return str(agents[agent_name])

    def get_clean_agent_name(self, url, html, existing_names=None):
        """
        Generates a clean, URL-based agent name derived from the HTML page title or URL path.

        Ensures that the name is URL-safe, lowercase, ASCII-only, hyphenated, and unique within the
        `existing_names` set. The result is truncated if necessary to stay within the allowed length.

        Args:
            url (str): The URL of the web page.
            html (str): The raw HTML content of the web page.
            existing_names (set, optional): A set of agent names already used. Ensures the result is unique.

        Returns:
            str: A clean, unique agent name suitable for use as an identifier.
        """
        if existing_names is None:
            existing_names = set()
        title = _extract_title_from_html(html)

        # If no title is found, fall back to using the URL path or netloc for the agent name
        if not title:
            parsed = urlparse(url)
            title = parsed.path.strip("/") or parsed.netloc

        base = sub(AGENT_NAME_SANITIZE_REGEX, " ", title).lower()
        base = sub(AGENT_NAME_HYPHENATE_REGEX, "-", base).strip("-")

        # Truncate cleanly to max allowed length
        base = base.encode("ascii", errors="ignore").decode()
        parts = base.split("-")

        # If already short, return it as-is
        if len("-".join(parts)) <= self.MAX_NAME_LEN:
            base = "-".join(parts)
        else:
            # Use a greedy truncation strategy by alternating between adding elements
            # from the start and end of the parts list, preserving meaning from both ends.
            front, back = [], []
            total_len = 2  # Accounts for the extra hyphen we'll add between front and back
            i, j = 0, len(parts) - 1
            turn = True  # Start by taking from the front

            while i <= j:
                candidate = parts[i] if turn else parts[j]
                # Check if adding this part would exceed the maximum name length
                if total_len + len(candidate) + 1 > self.MAX_NAME_LEN:
                    break
                if turn:
                    front.append(candidate)
                    i += 1
                else:
                    back.insert(0, candidate)  # Add to the beginning of the back list
                    j -= 1
                total_len += len(candidate) + 1
                turn = not turn  # Alternate between front and back

            base = "-".join(front + ["-"] + back)

        # If the name already exists, append a hash suffix to ensure uniqueness
        if base in existing_names:
            hash_suffix = md5(url.encode()).hexdigest()[:6]
            base = base[: self.MAX_NAME_LEN - 7].rstrip("-") + "-" + hash_suffix

        if len(base) > self.MAX_NAME_LEN:
            raise ValueError(f"Agent name too long: '{base}', len={len(base)}>{self.MAX_NAME_LEN}.")

        return base

    def _process_page(
        self,
        url: str,
        parent_name: Optional[str],
        resp,
        visited: set,
        existing_names: set,
        agents: dict,
        count: int,
        to_visit: List[Tuple[str, Optional[str]]],
        base_domain: str,
    ) -> int:
        text = clean_and_extract_text(resp.text)
        if len(text) < self.MIN_PAGE_LEN:
            return count  # Skip light pages

        name = self.get_clean_agent_name(url, resp.text, existing_names)
        existing_names.add(name)
        clean_text = (
            text[: self.PAGE_LEN_MAX].replace('"', "").replace("'", "").encode("ascii", errors="ignore").decode()
        )
        instructions = f"{self.AGENT_INSTRUCTION_PREFACE}\n\n{clean_text}".replace('"""', "").replace('"', "")

        if parent_name is None:
            is_top = "true"
            self.top_agent_name = name
        else:
            is_top = "false"
        self.add_agent(agents, name, instructions, [], is_top)

        if parent_name and parent_name in agents and name != parent_name:
            agents[parent_name].get("down_chains", []).append(name)

        visited.add(url)

        soup = BeautifulSoup(resp.text, "html.parser")
        for a in soup.find_all("a", href=True):
            full_link = urljoin(url, a["href"])
            if is_valid_url(full_link, base_domain) and full_link not in visited:
                if not any(full_link == queued_url for queued_url, _ in to_visit):
                    to_visit.append((full_link, name))

        return count + 1

    def crawl(self, start_url, max_agents):
        """
        Crawls a website starting from the given URL and constructs a hierarchy of content-based agents.

        Fetches HTML pages, extracts and cleans their textual content, and creates uniquely named agents
        representing each page. Links between agents are created based on internal navigation links.

        The crawl respects a maximum number of agents and skips pages with insufficient content. It also avoids
        revisiting URLs and ensures agent names are unique and well-formed. Pages that don’t meet content length
        requirements are excluded from the final network.

        Args:
            start_url (str): The root URL to begin crawling from.
            max_agents (int): Maximum number of agents (pages) to generate.

        Returns:
            dict: A dictionary representing the agent hierarchy, where each key is an agent name and each value is
                  a dictionary with "instructions", "down_chains", and "top_agent" fields.
        """
        agents = {}
        visited = set()
        to_visit: List[Tuple[str, Optional[str]]] = [(start_url, None)]
        count = 0
        # Use tldextract to isolate the registered domain and suffix (e.g., 'example.com') from the URL.
        # This helps in determining whether a link is internal to the site, which is important for focused crawling.
        domain_info = extract(start_url)
        base_domain = f"{domain_info.domain}.{domain_info.suffix}"
        existing_names = set()

        while to_visit and count < max_agents:
            url, parent_name = to_visit.pop(0)
            if url in visited:
                continue
            try:
                resp = get(url, timeout=10)
                # Skip non-HTML content types
                if "text/html" not in resp.headers.get("Content-Type", ""):
                    continue
                count = self._process_page(
                    url, parent_name, resp, visited, existing_names, agents, count, to_visit, base_domain
                )
            except (RequestException, ValueError, UnicodeDecodeError) as e:
                print(f"Skipping {url} due to error: {str(e)}")
                continue

            if hasattr(self, "politeness_delay") and self.politeness_delay > 0:
                sleep(uniform(self.politeness_delay * 0.75, self.politeness_delay * 1.25))

        linked = set()
        for agent in agents.values():
            linked.update(agent["down_chains"])

        print(f"Generated {count} agents with real content.")
        return agents

    @classmethod
    def main(cls):
        parser = ArgumentParser(description="Generate a hierarchy of web agents.")
        parser.add_argument(
            "--total_agents",
            type=int,
            default=cls.TOTAL_AGENTS,
            help=f"Total number of agents to generate (default: {cls.TOTAL_AGENTS})",
        )
        parser.add_argument(
            "--start_url", type=str, default=cls.START_URL, help=f"Starting URL (default: {cls.START_URL})"
        )
        parser.add_argument(
            "--agent_network_name",
            type=str,
            default=cls.AGENT_NETWORK_NAME,
            help=f"Agent network name (default: {cls.AGENT_NETWORK_NAME})",
        )
        parser.add_argument(
            "--max_children",
            type=int,
            default=cls.MAX_CHILDREN,
            help=f"Maximum number of direct children per agent (default: {cls.MAX_CHILDREN})",
        )
        parser.add_argument(
            "--max_name_len",
            type=int,
            default=cls.MAX_NAME_LEN,
            help=f"Maximum length for agent names (default: {cls.MAX_NAME_LEN})",
        )
        parser.add_argument(
            "--page_len_max",
            type=int,
            default=cls.PAGE_LEN_MAX,
            help=f"Maximum length of text content for a page (default: {cls.PAGE_LEN_MAX})",
        )
        parser.add_argument(
            "--min_page_len",
            type=int,
            default=cls.MIN_PAGE_LEN,
            help=f"Minimum required text length of a page (default: {cls.MIN_PAGE_LEN})",
        )
        parser.add_argument(
            "--politeness_delay",
            type=float,
            default=0.0,
            help="Average delay (in seconds) between page requests to be polite to servers (default: 0.0)",
        )

        args = parser.parse_args()

        # Dynamically set class attributes based on command-line arguments
        cls.MAX_CHILDREN = args.max_children
        cls.MAX_NAME_LEN = args.max_name_len
        cls.PAGE_LEN_MAX = args.page_len_max
        cls.MIN_PAGE_LEN = args.min_page_len

        the_start_url = args.start_url
        the_total_agents = args.total_agents
        the_agent_network_name = args.agent_network_name

        builder = cls()
        builder.politeness_delay = args.politeness_delay
        the_agents = builder.crawl(the_start_url, the_total_agents)
        the_agents = builder.enforce_fanout_recursive(the_agents, max_children=cls.MAX_CHILDREN)
        the_linked = set()
        for an_agnt in the_agents.values():
            the_linked.update(an_agnt.get("down_chains", []))

        # 1) single, safe hub insertion
        the_unlinked = []
        for a in the_agents:
            if a not in the_linked and the_agents[a].get("top_agent") == "false":
                the_unlinked.append(a)
        if the_unlinked:
            the_hub_name = "unlinked_hub"
            # only create once
            if the_hub_name not in the_agents:
                builder.add_agent(
                    the_agents, the_hub_name, "You connect otherwise unlinked agents.", the_unlinked, "false"
                )

            # Choose the top agent as the one with the most down_chains, excluding the unlinked hub
            candidates = {}
            for name, data in the_agents.items():
                if name != the_hub_name:
                    candidates[name] = data
            the_top = max(candidates.items(), key=lambda x: len(x[1].get("down_chains", [])))[0]
            # guard against self-reference and double-appending
            if the_hub_name != the_top and the_hub_name not in the_agents[the_top].get("down_chains", []):
                the_agents[the_top].setdefault("down_chains", []).append(the_hub_name)

        # 2) FINAL SANITY: strip any remaining self-references
        for name, data in the_agents.items():
            data["down_chains"] = [child for child in data.get("down_chains", []) if child != name]

        # now generate HOCON
        hocon = get_agent_network_hocon(the_agents, the_agent_network_name)

        # Write the agent network file
        from pathlib import Path

        file_path = Path(cls.OUTPUT_PATH) / f"{the_agent_network_name}.hocon"
        # Ensure the directory exists
        makedirs(file_path.parent, exist_ok=True)
        with file_path.open("w", encoding="utf-8") as file:
            file.write(hocon)
        print(f"\n agent count: {builder.agent_counter}")
        print("\nDone!\n")


def is_valid_url(link, base_domain):
    """
    Determines whether a given link is a valid internal HTTP/HTTPS URL within the specified base domain.

    Args:
        link (str): The URL to validate.
        base_domain (str): The base domain that the link must belong to (e.g., "example.com").

    Returns:
        bool: True if the link is a valid internal HTTP/HTTPS link for the domain, False otherwise.
    """
    parsed = urlparse(link)
    return parsed.scheme in ("http", "https") and base_domain in parsed.netloc


def clean_and_extract_text(html):
    """
    Cleans and extracts readable text content from HTML.

    Removes non-content elements (e.g., scripts, styles, images), strips unnecessary attributes,
    extracts visible text from paragraph-level tags, and sanitizes the text by removing URLs,
    special characters, and non-ASCII content.

    Args:
        html (str): Raw HTML content of a web page.

    Returns:
        str: Cleaned and normalized text extracted from the HTML.
    """
    soup = BeautifulSoup(html, "html.parser")

    # Remove unwanted tags entirely
    for tag in soup(["script", "style", "noscript", "img", "source", "picture", "svg"]):
        tag.decompose()

    # Remove image srcset and media nonsense in attributes
    for tag in soup.find_all(True):
        for attr in ["src", "srcset", "data-src", "data-srcset", "alt", "title"]:
            if attr in tag.attrs:
                del tag.attrs[attr]

    # Extract visible paragraph-level content
    paragraphs = soup.find_all(["p", "h1", "h2", "h3", "li"])
    raw_text = " ".join(p.get_text(separator=" ", strip=True) for p in paragraphs)

    # Remove URLs or scene7 junk using regex
    clean_text = sub(URL_REGEX, "", raw_text)
    clean_text = sub(SCENE7_JUNK_REGEX, "", clean_text)

    # Reminder: PAGE_LEN_MAX (default 5000) truncates page content earlier in crawl();
    # if changing size logic, update both places or consolidate here.
    # Normalize to ascii-only and strip quotes
    clean_text = clean_text.replace('"', "").replace("'", "")
    clean_text = clean_text.encode("ascii", errors="ignore").decode()

    return clean_text.strip()


def _extract_title_from_html(html: str) -> str:
    """
    Extracts the title from the given HTML content using BeautifulSoup.

    This method encapsulates the HTML parsing logic, allowing easier reuse or replacement
    with a different parser if needed in the future.

    Args:
        html (str): Raw HTML content.

    Returns:
        str: The cleaned title string if found, otherwise an empty string.
    """
    soup = BeautifulSoup(html, "html.parser")
    return soup.title.string.strip() if soup.title and soup.title.string else ""


def random_id(prefix="", length=6):
    """
    Generates a random alphanumeric identifier with an optional prefix.

    The identifier consists of lowercase letters and digits, with a default total
    length of 6 characters (excluding the prefix).

    Args:
        prefix (str): Optional string to prepend to the random ID.
        length (int): Length of the random portion of the ID (default is 6).

    Returns:
        str: A string representing the generated ID.
    """
    return prefix + "".join(choices(ascii_lowercase + digits, k=length))


def get_agent_network_hocon(agents, agent_network_name):
    """
    Converts the agent hierarchy dictionary into a HOCON-formatted string.

    Ensures that one agent is marked as the top agent (if not already set),
    formats each agent entry according to its type (top, regular, or leaf),
    and constructs a valid HOCON representation of the entire network.

    Args:
        agents (dict): The dictionary containing all agents with their attributes ("instructions", "down_chains",
        "top_agent").
        agent_network_name (str): The name of the agent network, used as the root identifier in the HOCON output.

    Returns:
        str: A HOCON-formatted string representing the complete agent network.
    """
    # If a top agent has already been designated, explicitly mark it in the agent dictionary
    if hasattr(WebAgentNetworkBuilder, "top_agent_name") and WebAgentNetworkBuilder.top_agent_name:
        top_agent_name = WebAgentNetworkBuilder.top_agent_name
        if top_agent_name in agents:
            agents[top_agent_name]["top_agent"] = "true"
            print(f"Assigned top_agent to: {top_agent_name}")

    # Initialize HOCON string with the standard header
    agent_network_hocon = HOCON_HEADER_START + agent_network_name + HOCON_HEADER_REMAINDER
    for agent_name, agent in agents.items():
        unique_tools = []
        # Deduplicate and validate the down_chains list for each agent
        for down_chain in agent.get("down_chains", []):
            if down_chain == agent_name:
                print(f"Warning: Agent '{agent_name}' directly references itself in down_chains.")
                continue  # Skip it entirely
            elif down_chain not in unique_tools:
                unique_tools.append(f'"{down_chain}"')

        tools = ",".join(unique_tools)

        # Format the agent using the appropriate HOCON template based on its type
        if agent.get("top_agent") == "true":
            an_agent = TOP_AGENT_TEMPLATE % (
                agent_name,
                agent["instructions"],
                tools,
            )
        elif agent.get("down_chains"):
            an_agent = REGULAR_AGENT_TEMPLATE % (
                agent_name,
                agent["instructions"],
                tools,
            )
        else:
            an_agent = LEAF_NODE_AGENT_TEMPLATE % (
                agent_name,
                agent["instructions"],
            )
        agent_network_hocon += an_agent

    # Finalize the HOCON string with a closing bracket
    agent_network_hocon += "]\n}\n"

    return agent_network_hocon


if __name__ == "__main__":
    WebAgentNetworkBuilder().main()
