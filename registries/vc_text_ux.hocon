
# Copyright (C) 2023-2025 Cognizant Digital Business, Evolutionary AI.
# All Rights Reserved.
# Issued under the Academic Public License.
#
# You can be released from the terms, and requirements of the Academic Public
# License by purchasing a commercial license.
# Purchase of a commercial license is mandatory for any use of the
# neuro-san SDK Software in commercial settings.
#
# END COPYRIGHT
{
    include "registries/aaosa.hocon"

    "llm_config": {
        "use_model": "gpt-4o",
    },
    "max_iterations": 40000,
    "max_execution_seconds": 6000,
    "grounding_instructions": """
Follow this rubric to arrive at reasonable scores:
## 1–30: Poor or Minimal - Lacks clarity, detail, or seriousness, Highly generic or boilerplate, Fails to meet basic expectations
## 31–50: Below Average - Some effort or relevance, but falls short, Incomplete, unoriginal, or weakly presented
## 51–70: Average to Good - Adequately meets expectations, Some originality, functional execution
## 71–89: Strong - Thoughtfully designed or articulated, Technically sound, strategically solid
## 90–100: Exceptional - Highly novel, ambitious, or impactful, Breakthrough-level execution, Rare level of excellence
## SCORING POLICY
    - Penalize vague or underspecified responses
    - Exceptional scores (>90) should be rare.
    - Scores below 50 are valid and necessary for weak entries
    - Use the full 1–100 scale when evaluating. Do not cluster scores around 60–80 by default.
    - Scores cannot be None or null.
    """,
    "aaosa_instructions_suffix": """
**IMPORTANT:** Invoke your 'evaluate_score' tool with the full text as 'inquiry' to evaluate the inputs.
If the returned score is None or null or empty, invoke the 'evaluate_score' tool again.
    """,

    "tools": [
        # This first agent definition is regarded as the "Front Man", which
        # does all the talking to the outside world/client.
        #
        # Some disqualifications from being a front man:
        #   1) Cannot use a CodedTool "class" definition
        #   2) Cannot use a Tool "toolbox" definition
        #
        # Besides the first agent being the front man, these tool definitions
        # do not have to be in any particular order. How they are linked and
        # call each other is defined within their own specs.
        # This could be a graph, potentially even with cycles.
        // The first agent
        // Create evaluation
        {
            "name": "create_eval",
            "function": {
                "description": "I can help you evaluate vibe coding ideas."
            },
            "instructions": """
You are the evaluation orchestrator for a vibe-coding idea.

ALWAYS call 'evaluate_score' to evaluate an idea.
            """ ${aaosa_instructions} ${aaosa_instructions_suffix},
            "allow": {
                "to_upstream": {
                    "sly_data": {
                        "evaluation": true,
                    }
                }
            },
            "tools": ["evaluate_score"]
        },
        # Evaluate Innovation
        {
            "name": "evaluate_score",
            "function": ${aaosa_call},
            "instructions": """
You are a UX designer and human‑computer interaction expert evaluating the User Experience (UX) of the vibe‑coding idea submission based on the input.
Always use your tools to evaluate the inputs.

Step 1:
""" ${grounding_instructions} """

Evaluate the following aspects:  
For each of the following 10 sub‑criteria, give a score from 1 to 100 (1 = very poor, 100 = excellent):
    - Ease of use (goals discoverable and achievable)
    - Interface clarity (UI, CLI, or API intuitiveness)
    - Onboarding and learnability (ease for new users)
    - Documentation quality (user/dev guidance)
    - Interaction design (accessibility, responsiveness, visual hierarchy)
    - Error prevention and handling (feedback, safeguards)
    - Accessibility and inclusivity (multilingual, screen readers, keyboard-only)
    - User feedback and iteration evidence (if mentioned)
    - Consistency across touchpoints (UI patterns, predictable flows)
    - Overall satisfaction potential (how pleasant and efficient the experience seems)

Step 2:
Draft the following json dict:
{{
  "score": <[a list of above 10 scores, all between 1 and 100]>,
  "brief_description": "<Provide a brief description of your scoring rationale.>"
}}

Step 3:
**IMPORTANT:** Always Call the 'manage_eval' tool with above json to store the evaluation results.
            """,
            "command": """
Call the 'manage_eval' tool and return the json block with following fields:
    {{
    "score": <number between 1 and 100 (1 = not innovative at all, 100 = exceptionally novel and differentiated)>,
    "brief_description": "<brief description of your scoring rationale>"
    }}
            """,
            "tools": ["manage_eval"]
        },
        // Manage idea evaluation
        {
            "name": "manage_eval",
            "function": {
                "description": "I can update the evaluation with the scores and descriptions provided by the other tools.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "score": {
                            "type": "array",
                            "items": {
                                "type": "float",
                                },
                            "description": "a list of scores for innovation, between 1 and 100."
                        },
                        "brief_description": {
                            "type": "string",
                            "description": "A reasoning as to why we have the given score."
                        }
                    },
                }
            },
            "class": "vc_manage_eval.ManageEval"
        },
    ]
}
